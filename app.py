import streamlit as st
import os
import openai
import anthropic
import re

def transcribe_audio(gpt_client, file_path, model_name="whisper-1", prompt_text=None):
    with open(file_path, "rb") as audio_file:
        transcription = gpt_client.audio.transcriptions.create(
            model=model_name,
            file=audio_file,
            prompt=prompt_text
        )
    return transcription.text


def transcript_to_markdown(client, transcript, example_transcripts, example_markdowns, model_id):
    system_prompt = "æ‚¨æ˜¯ä¸€ä½å°ˆæ¥­ä½œå®¶ã€‚æ‚¨çš„ä»»å‹™æ˜¯å°‡è¨ªè«‡é€å­—ç¨¿è½‰æ›ç‚ºçµæ§‹è‰¯å¥½ã€æ­£å¼ä¸”å¼•äººå…¥å‹çš„ç¹é«”ä¸­æ–‡éƒ¨è½æ ¼æ–‡ç« ï¼Œä¸¦ä»¥ Markdown æ ¼å¼å‘ˆç¾ã€‚"
    
    # Construct the prompt with examples
    prompt = (
        "æ‚¨çš„ä»»å‹™æ˜¯å°‡ä»¥ä¸‹è¨ªè«‡é€å­—ç¨¿è½‰æ›ç‚ºä¸€ç¯‡æ½¤é£¾éã€çµæ§‹è‰¯å¥½çš„ Markdown æ ¼å¼éƒ¨è½æ ¼æ–‡ç« ã€‚ "
        "è«‹ä»”ç´°éµå¾ªä»¥ä¸‹èªªæ˜ï¼Œä»¥ç¢ºä¿è¼¸å‡ºç¬¦åˆæ‰€éœ€æ¨™æº–ï¼š\n\n"
        "1.  **åˆ†æç¯„ä¾‹ï¼š** å¯©æŸ¥æä¾›çš„ç¯„ä¾‹é€å­—ç¨¿åŠå…¶å°æ‡‰çš„æ–‡ç« ï¼Œä»¥äº†è§£æ‰€éœ€çš„èªæ°£ã€é¢¨æ ¼å’Œçµæ§‹ã€‚\n"
        "2.  **ä¿æŒæ­£å¼ä¸”å¼•äººå…¥å‹çš„èªæ°£ï¼š** æ–‡ç« æ‡‰å°ˆæ¥­ä¸”æ˜“æ–¼é–±è®€ï¼Œé¿å…éæ–¼éš¨æ„çš„èªè¨€æˆ–ç°¡å–®çš„é€å­—è½‰éŒ„ã€‚\n"
        "3.  **æ–‡ç« çµæ§‹ï¼š** é‚è¼¯æ€§åœ°çµ„ç¹”å…§å®¹ï¼Œä½¿ç”¨æ¸…æ™°çš„æ¨™é¡Œã€æ®µè½å’Œå…¶ä»– Markdown å…ƒç´ ä»¥æé«˜å¯è®€æ€§ã€‚\n"
        "4.  **åƒ…è¼¸å‡ºæ–‡ç« ï¼š** æ‚¨çš„æœ€çµ‚è¼¸å‡ºæ‡‰åƒ…ç‚ºç”Ÿæˆçš„æ–‡ç« ï¼Œä¸¦åŒ…å«åœ¨ <article> å’Œ </article> æ¨™ç±¤å…§ã€‚è«‹å‹¿åœ¨æ¨™ç±¤ä¹‹å‰æˆ–ä¹‹å¾ŒåŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—æˆ–è§£é‡‹ã€‚\n\n"
    )
    
    for i in range(len(example_transcripts)):
        prompt += f"<example_transcript_{i+1}>\n{example_transcripts[i]}\n</example_transcript_{i+1}>\n\n"
        prompt += f"<example_article_{i+1}>\n{example_markdowns[i]}\n</example_article_{i+1}>\n\n"

    prompt += (
        f"<transcript>\n{transcript}\n</transcript>\n\n"
        "è«‹æ ¹æ“šæä¾›çš„é€å­—ç¨¿ç”Ÿæˆæ–‡ç« ï¼Œä¸¦éµå¾ªä¸Šè¿°æ‰€æœ‰èªªæ˜å’Œç¯„ä¾‹ã€‚ "
        "è«‹å°‡æ‚¨çš„æœ€çµ‚è¼¸å‡ºåŒ…å«åœ¨ <article> å’Œ </article> æ¨™ç±¤å…§ã€‚"
    )

    message = [
        {"role": "user", "content": [{"type": "text", "text": prompt}]}
    ]

    if "claude" in model_id:
        response = client.messages.create(
            model=model_id,
            max_tokens=10000,
            system=system_prompt,
            messages=message,
            temperature=1.0
        )
        article_content = re.search(r'<article>(.*?)</article>', response.content[0].text, re.DOTALL)
    elif "gpt" in model_id:
        response = client.chat.completions.create(
            model=model_id,
            messages=[{"role": "system", "content": system_prompt}] + message,
            max_tokens=10000,
            temperature=1.0
        )
        article_content = re.search(r'<article>(.*?)</article>', response.choices[0].message.content, re.DOTALL)
    else:
        raise ValueError("ä¸æ”¯æ´çš„æ¨¡å‹ IDã€‚")

    
    if article_content:
        return article_content.group(1).strip()
    else:
        # For Claude, response.content[0].text contains the full response
        # For OpenAI, response.choices[0].message.content contains the full response
        full_response_content = response.content[0].text if "claude" in model_id else response.choices[0].message.content
        return ("AI çš„å›æ‡‰æœªéµå¾ªæŒ‡ç¤ºã€‚ä»¥ä¸‹æ˜¯å®Œæ•´çš„å›æ‡‰ã€‚ "
                "è«‹å°‹æ‰¾ <article> æ¨™ç±¤ä»¥æ‰¾åˆ°ç”Ÿæˆçš„æ–‡ç« ã€‚ "
                "å…§å®¹ç‚º Markdown æ ¼å¼ï¼Œå¯åœ¨ä»¥ä¸‹ç¶²å€é è¦½ï¼šhttps://claude.site/artifacts/39f78d93-52a9-48ee-90ac-0732c48f9835\n\n"
                f"{full_response_content}")

def generate_article_from_transcript(client, transcript, model):
    # Load example transcripts and markdowns dynamically
    example_transcript_dir = "./example_transcripts"
    example_markdown_dir = "./example_markdowns"

    example_transcripts = []
    for filename in sorted(os.listdir(example_transcript_dir)):
        if filename.endswith(".txt"):
            with open(os.path.join(example_transcript_dir, filename), "r", encoding="utf-8") as f:
                example_transcripts.append(f.read())

    example_markdowns = []
    for filename in sorted(os.listdir(example_markdown_dir)):
        if filename.endswith(".md"):
            with open(os.path.join(example_markdown_dir, filename), "r", encoding="utf-8") as f:
                example_markdowns.append(f.read())

    with st.spinner('å¯«æ‰‹è¶•ç¨¿ä¸­...'):
        markdown = transcript_to_markdown(client, transcript, example_transcripts, example_markdowns, model)
    st.divider()
    st.subheader("æ–‡ç« :")
    st.markdown(markdown)
    


def main():
    st.sidebar.title("è¨­å®š")
    openai_api_key = st.sidebar.text_input("OpenAI API é‡‘é‘°", type="password")
    claude_api_key = st.sidebar.text_input("Claude API é‡‘é‘°", type="password")
    password = st.sidebar.text_input("ä½©ä½©å°ˆå±¬å¯†ç¢¼", placeholder="XXXæœ€å¸¥")

    st.sidebar.title("æ¨¡å‹é¸æ“‡")
    model_options = {
        "Claude Sonnet 4 (default)": "claude-sonnet-4-20250514",
        "Claude Opus 4": "claude-opus-4-20250514",
        "GPT-4o mini": "gpt-4o-mini-2024-07-18",
        "GPT-4.1": "gpt-4.1-2025-04-14"
    }
    selected_model_name = st.sidebar.selectbox("é¸æ“‡ä¸€å€‹æ¨¡å‹ï¼š", list(model_options.keys()))
    model_id = model_options[selected_model_name]

    gpt_client = openai.OpenAI(api_key=openai_api_key)

    if openai_api_key and claude_api_key:
        if "gpt" in model_id:
            client = openai.OpenAI(api_key=openai_api_key)
        elif "claude" in model_id:
            client = anthropic.Anthropic(api_key=claude_api_key)
        else:
            st.error("ç„¡æ•ˆçš„æ¨¡å‹é¸æ“‡ã€‚")
            st.stop()

        st.title("ä½©ä½©è«‡è©±é£Ÿé–“å°ˆç”¨ï¼šPodcast è½‰æ–‡ç« å°å·¥å…·")

        # Create tabs
        tab1, tab2, tab3 = st.tabs(["MP3", "é€å­—ç¨¿", "æ¨¡å‹"])

        with tab1:
            st.markdown('<span style="color: grey;">éŸ³æª”å¿…é ˆå°æ–¼25MBï¼Œè«‹å°‡éŸ³æª”å…ˆå£“ç¸®å¾Œå†ä¸Šå‚³ï¼Œhttps://www.freeconvert.com/mp3-compressor</span>', unsafe_allow_html=True)
            uploaded_file = st.file_uploader("ä¸Šå‚³ MP3 æª”æ¡ˆ", type=["mp3"])
            
            prompt_text = st.text_input("è«‹è¼¸å…¥éŸ³æª”ä¸­å¯èƒ½å‡ºç¾çš„å°ˆæœ‰åè©ï¼Œå¹«åŠ©é€å­—ç¨¿çš„æº–ç¢ºåº¦", "è«‡è©±é£Ÿé–“, ä½©ä½©, (æ–°å¢é—œéµå­—)")

            if st.button("ç”Ÿæˆ"):
                if password == "å‘¨æ°å€«æœ€å¸¥":
                    if uploaded_file is not None:
                        with st.status("è™•ç†ä¸­...") as status:
                            status.write("æ­£åœ¨ä¸Šå‚³éŸ³æª”...")
                            os.makedirs("podcasts", exist_ok=True)
                            audio_path = os.path.join("podcasts", uploaded_file.name)
                            with open(audio_path, "wb") as f:
                                f.write(uploaded_file.getbuffer())
                            st.success("éŸ³æª”ä¸Šå‚³æˆåŠŸã€‚")

                            status.write("æ­£åœ¨è½‰éŒ„éŸ³æª”ï¼ˆé€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜ï¼‰...")
                            transcript = transcribe_audio(gpt_client, audio_path, prompt_text=prompt_text)
                            st.subheader("é€å­—ç¨¿ï¼š")
                            st.text_area("é€å­—ç¨¿", transcript, height=300)

                            status.write(f"æ­£åœ¨ç”Ÿæˆæ–‡ç« ... (æ¨¡å‹: {model_id})")
                            generate_article_from_transcript(client, transcript, model_id)
                            status.update(label="å®Œæˆï¼", state="complete")

                    else:
                        st.error("è«‹ä¸Šå‚³ MP3 æª”æ¡ˆã€‚")
                else:
                    st.error("èª°æœ€å¸¥å‘€ï¼Ÿèªªäº†æ‰èƒ½ç”¨å–”ï¼ğŸ˜‰")
            

        with tab2:
            transcript_input = st.text_area("åœ¨æ­¤è²¼ä¸Šæ‚¨çš„é€å­—ç¨¿ï¼š", height=300)
            if st.button("å¾é€å­—ç¨¿ç”Ÿæˆ"):
                if password == "å‘¨æ°å€«æœ€å¸¥":
                    if transcript_input:
                        with st.status("è™•ç†ä¸­...") as status:
                            status.write(f"æ­£åœ¨ç”Ÿæˆæ–‡ç« ... (æ¨¡å‹: {model_id})")
                            generate_article_from_transcript(client, transcript_input, model_id)
                            status.update(label="å®Œæˆï¼", state="complete")
                    else:
                        st.error("è«‹æä¾›é€å­—ç¨¿ã€‚")
                else:
                    st.error("èª°æœ€å¸¥å‘€ï¼Ÿèªªäº†æ‰èƒ½ç”¨å–”ï¼ğŸ˜‰")

        with tab3:
            st.markdown("""
            ### æ¨¡å‹è³‡è¨Š

            é€™æ˜¯ä¸€å€‹é—œæ–¼æœ¬æ‡‰ç”¨ç¨‹å¼ä¸­å¯ç”¨çš„ä¸åŒå¤§å‹èªè¨€æ¨¡å‹çš„å¿«é€ŸæŒ‡å—ã€‚

            ---

            #### **Anthropic Claude**

            **Claude Sonnet 4 (default)**
            - **è²»ç”¨**: $3 / ç™¾è¬è¼¸å…¥ tokens, $15 / ç™¾è¬è¼¸å‡º tokens
            - **å„ªé»**:
              - ç†æƒ³çš„å¹³è¡¡äº†æ™ºæ…§èˆ‡é€Ÿåº¦ï¼Œç‰¹åˆ¥æ˜¯åœ¨è™•ç†è¤‡é›œçš„ä»»å‹™æ™‚ã€‚
              - åœ¨éœ€è¦å¼·å¤§ç†è§£èƒ½åŠ›å’Œå¿«é€Ÿåæ‡‰çš„ä¼æ¥­ç´šæ‡‰ç”¨ä¸­è¡¨ç¾å‡ºè‰²ã€‚
              - é©ç”¨æ–¼è³‡æ–™åˆ†æã€ç´°ç·»å…§å®¹å‰µä½œå’Œç¨‹å¼ç¢¼ç”Ÿæˆã€‚

            **Claude Opus 4**
            - **è²»ç”¨**: $15 / ç™¾è¬è¼¸å…¥ tokens, $75 / ç™¾è¬è¼¸å‡º tokens
            - **å„ªé»**:
              - æœ€å¼·å¤§çš„æ¨¡å‹ï¼Œé©ç”¨æ–¼è™•ç†é«˜åº¦è¤‡é›œçš„ä»»å‹™ã€‚
              - åœ¨éœ€è¦æ·±åº¦æ¨ç†ã€ç­–ç•¥è¦åŠƒå’Œè¤‡é›œå•é¡Œè§£æ±ºçš„å ´æ™¯ä¸­è¡¨ç¾å“è¶Šã€‚
              - é©åˆç ”ç™¼ã€é€²éšåˆ†æå’Œéœ€è¦é ‚ç´šæ™ºæ…§çš„ä»»å‹™ã€‚

            ---

            #### **OpenAI GPT**

            **GPT-4o mini**
            - **è²»ç”¨**: $0.15 / ç™¾è¬è¼¸å…¥ tokens, $0.60 / ç™¾è¬è¼¸å‡º tokens
            - **å„ªé»**:
              - ç‚ºé€Ÿåº¦å’Œæˆæœ¬é€²è¡Œäº†å„ªåŒ–ï¼Œæ˜¯éœ€è¦å¿«é€Ÿå›æ‡‰çš„ä»»å‹™çš„ç†æƒ³é¸æ“‡ã€‚
              - é©ç”¨æ–¼èŠå¤©æ©Ÿå™¨äººã€å…§å®¹æ‘˜è¦å’Œå³æ™‚ç¿»è­¯ã€‚

            **GPT-4.1**
            - **è²»ç”¨**: $5 / ç™¾è¬è¼¸å…¥ tokens, $15 / ç™¾è¬è¼¸å‡º tokens (æ­¤ç‚º GPT-4o åƒ¹æ ¼ï¼Œåƒ…ä¾›åƒè€ƒ)
            - **å„ªé»**:
              - OpenAI æœ€æ–°çš„ GPT-4 æ¨¡å‹ï¼Œå…·æœ‰æ›´é«˜çš„æ™ºæ…§å’Œæ”¹é€²çš„æ•ˆèƒ½ã€‚
              - é©ç”¨æ–¼éœ€è¦é«˜æº–ç¢ºåº¦å’Œç†è§£ç´°å¾®å·®åˆ¥çš„è¤‡é›œä»»å‹™ã€‚
            """)

if __name__ == "__main__":
    main()